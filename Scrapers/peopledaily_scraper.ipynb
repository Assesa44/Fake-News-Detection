{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5002513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Extracting article links...\n",
      "üì∞ Visiting 53 articles...\n",
      "\n",
      "‚úÖ Done! Scraped 42 full articles.\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Start undetected Chrome browser\n",
    "options = uc.ChromeOptions()\n",
    "options.headless = True  # Run headless (no browser window)\n",
    "\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# Target site\n",
    "base_url = \"https://peopledaily.digital/\"\n",
    "driver.get(base_url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Collect article URLs and titles\n",
    "articles = driver.find_elements(By.CSS_SELECTOR, \"a[href^='https://peopledaily.digital/']\")\n",
    "article_data = []\n",
    "\n",
    "print(\"üîé Extracting article links...\")\n",
    "\n",
    "for link in articles:\n",
    "    href = link.get_attribute('href')\n",
    "    title = link.text.strip()\n",
    "    \n",
    "    if title and href and \"page\" not in href.lower() and \"/tag/\" not in href.lower():\n",
    "        article_data.append({\"title\": title, \"url\": href})\n",
    "\n",
    "unique_articles = {item['url']: item for item in article_data}.values()\n",
    "\n",
    "print(f\"üì∞ Visiting {len(unique_articles)} articles...\")\n",
    "\n",
    "final_data = []\n",
    "\n",
    "for article in unique_articles:\n",
    "    try:\n",
    "        driver.get(article['url'])\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Get article date\n",
    "        try:\n",
    "            date_element = driver.find_element(By.CLASS_NAME, \"content--date--date-time\")\n",
    "            date = date_element.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            date = None\n",
    "\n",
    "        # Get article content\n",
    "        try:\n",
    "            content_div = driver.find_element(By.CSS_SELECTOR, \"div.col-md-7.content\")\n",
    "            paragraphs = content_div.find_elements(By.TAG_NAME, \"p\")\n",
    "            content = \" \".join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "        except NoSuchElementException:\n",
    "            content = None\n",
    "\n",
    "        if content:\n",
    "            final_data.append({\n",
    "                \"source\": \"People Daily\",\n",
    "                \"title\": article['title'],\n",
    "                \"url\": article['url'],\n",
    "                \"date\": date,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\"‚è±Ô∏è Timeout at {article['url']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error at {article['url']}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(final_data)\n",
    "print(f\"\\n‚úÖ Done! Scraped {len(df)} full articles.\")\n",
    "\n",
    "df.to_csv(\"peopledaily_articles_uc.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7c52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/RawData/people_daily_articles.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
